{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a60d09a",
   "metadata": {},
   "source": [
    "# GNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e0e096",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddbbdc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import io\n",
    "import sys\n",
    "import toml\n",
    "import pprint\n",
    "import importlib\n",
    "\n",
    "# Scientific stack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# JARVIS dataset utilities\n",
    "from jarvis.db.figshare import data\n",
    "\n",
    "# Plotting setup\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Progess bar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9d9bca",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33f06999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Configuration:\n",
      "{'data': {'dataset_name': 'dft_3d', 'store_dir': '/shared/data/jarvis'},\n",
      " 'filters': {'bandgap_column': 'optb88vdw_bandgap',\n",
      "             'ehull': 0.1,\n",
      "             'max_eps': 10.0,\n",
      "             'min_eps': 1.0,\n",
      "             'semiconductor_max': 4.0,\n",
      "             'semiconductor_min': 0.5,\n",
      "             'toxic_elements': ['Pb', 'Cd', 'Hg', 'As', 'Se'],\n",
      "             'transparent_min': 3.0},\n",
      " 'logging': {'enable': True,\n",
      "             'file': '/shared/data/jarvis/project_multimodal.log',\n",
      "             'level': 'INFO'},\n",
      " 'model': {'categorical': {'crys': {'embed_dim': 4, 'vocab_size': 8},\n",
      "                           'dimensionality': {'embed_dim': 4, 'vocab_size': 8},\n",
      "                           'func': {'embed_dim': 1, 'vocab_size': 2},\n",
      "                           'spg_number': {'embed_dim': 50, 'vocab_size': 214},\n",
      "                           'typ': {'embed_dim': 1, 'vocab_size': 2}},\n",
      "           'devices': {'model': 'cuda'},\n",
      "           'formula': {'embed_dim': 32, 'num_elements': 89},\n",
      "           'graph': {'atom_embed_dim': 64,\n",
      "                     'edge_feat_dim': 4,\n",
      "                     'max_neighbors': 24,\n",
      "                     'radius': 5.0},\n",
      "           'mlp': {'dropout': 0.3, 'hidden1': 128, 'hidden2': 64},\n",
      "           'num_cols': ['formation_energy_peratom',\n",
      "                        'optb88vdw_bandgap',\n",
      "                        'slme',\n",
      "                        'magmom_oszicar',\n",
      "                        'spillage',\n",
      "                        'kpoint_length_unit',\n",
      "                        'optb88vdw_total_energy',\n",
      "                        'epsx',\n",
      "                        'epsy',\n",
      "                        'epsz',\n",
      "                        'density',\n",
      "                        'poisson',\n",
      "                        'nat',\n",
      "                        'bulk_modulus_kv',\n",
      "                        'shear_modulus_gv',\n",
      "                        'mbj_bandgap',\n",
      "                        'hse_gap',\n",
      "                        'ehull',\n",
      "                        'Tc_supercon'],\n",
      "           'threshold': 0.94,\n",
      "           'training': {'batch_size': 32, 'epochs': 20}},\n",
      " 'project': {'author': 'Brent Allen Thorne',\n",
      "             'name': 'Transparent Nontoxic Semiconductors (GNN+CNN)',\n",
      "             'version': '1.0'},\n",
      " 'system': {'path': './'}}\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from TOML file\n",
    "CONFIG_PATH = \"config_multimodal.toml\"\n",
    "config = toml.load(CONFIG_PATH)\n",
    "\n",
    "print(\"Project Configuration:\")\n",
    "pprint.pprint(config)\n",
    "\n",
    "# Add system path for custom modules\n",
    "SYS_PATH = config.get('system', {}).get('path', './')\n",
    "sys.path.append(SYS_PATH)  # e.g., .../code/jarvis/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f50698",
   "metadata": {},
   "source": [
    "## Custom Utilities and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af423f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 20:11:26,379 - jarvis_project - INFO - Project configuration loaded.\n",
      "2025-12-10 20:11:26,381 - jarvis_project - INFO - Dataset: dft_3d\n",
      "2025-12-10 20:11:26,382 - jarvis_project - INFO - Store directory: /shared/data/jarvis\n",
      "2025-12-10 20:11:28,446 - jarvis_project - INFO - Dataset shape: (75993, 64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (75993, 64)\n"
     ]
    }
   ],
   "source": [
    "from jarvis_utils import load_or_fetch_dataset\n",
    "from logger_utils import setup_logger, flush_logger\n",
    "\n",
    "# Setup logger\n",
    "logger = setup_logger(config)\n",
    "logger.info(\"Project configuration loaded.\")\n",
    "logger.info(f\"Dataset: {config['data']['dataset_name']}\")\n",
    "logger.info(f\"Store directory: {config['data']['store_dir']}\")\n",
    "\n",
    "# Load dataset\n",
    "df = load_or_fetch_dataset(config[\"data\"][\"dataset_name\"], data, config[\"data\"][\"store_dir\"])\n",
    "logger.info(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5177b09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['jid', 'spg_number', 'spg_symbol', 'formula',\n",
       "       'formation_energy_peratom', 'func', 'optb88vdw_bandgap', 'atoms',\n",
       "       'slme', 'magmom_oszicar', 'spillage', 'elastic_tensor',\n",
       "       'effective_masses_300K', 'kpoint_length_unit', 'maxdiff_mesh',\n",
       "       'maxdiff_bz', 'encut', 'optb88vdw_total_energy', 'epsx', 'epsy', 'epsz',\n",
       "       'mepsx', 'mepsy', 'mepsz', 'modes', 'magmom_outcar', 'max_efg',\n",
       "       'avg_elec_mass', 'avg_hole_mass', 'icsd', 'dfpt_piezo_max_eij',\n",
       "       'dfpt_piezo_max_dij', 'dfpt_piezo_max_dielectric',\n",
       "       'dfpt_piezo_max_dielectric_electronic',\n",
       "       'dfpt_piezo_max_dielectric_ionic', 'max_ir_mode', 'min_ir_mode',\n",
       "       'n-Seebeck', 'p-Seebeck', 'n-powerfact', 'p-powerfact', 'ncond',\n",
       "       'pcond', 'nkappa', 'pkappa', 'ehull', 'Tc_supercon', 'dimensionality',\n",
       "       'efg', 'xml_data_link', 'typ', 'exfoliation_energy', 'spg', 'crys',\n",
       "       'density', 'poisson', 'raw_files', 'nat', 'bulk_modulus_kv',\n",
       "       'shear_modulus_gv', 'mbj_bandgap', 'hse_gap', 'reference', 'search'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46898e",
   "metadata": {},
   "source": [
    "## Configuration and feature schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97f36ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Numeric feature columns\\nnum_cols = [\\n    \"formation_energy_peratom\", \"optb88vdw_bandgap\",\\n    \"slme\", \"magmom_oszicar\", \"spillage\", \"kpoint_length_unit\",\\n    \"optb88vdw_total_energy\", \"epsx\", \"epsy\", \"epsz\", \"density\",\\n    \"poisson\", \"nat\", \"bulk_modulus_kv\", \"shear_modulus_gv\",\\n    \"mbj_bandgap\", \"hse_gap\", \"ehull\", \"Tc_supercon\"\\n]\\n\\n# Unified configuration\\nconfig_model = {\\n    \"devices\": {\"model\": \"cuda\"},\\n    \"threshold\": 0.94,\\n\\n    \"numeric_cols\": num_cols,\\n\\n    \"categorical\": {\\n        \"spg_number\": {\"vocab_size\": 214, \"embed_dim\": 50},\\n        \"func\": {\"vocab_size\": 2, \"embed_dim\": 1},\\n        \"dimensionality\": {\"vocab_size\": 8, \"embed_dim\": 4},\\n        \"typ\": {\"vocab_size\": 2, \"embed_dim\": 1},\\n        \"crys\": {\"vocab_size\": 8, \"embed_dim\": 4},\\n    },\\n\\n    \"formula\": {\"num_elements\": 89, \"embed_dim\": 32},\\n\\n    \"mlp\": {\"hidden1\": 128, \"hidden2\": 64, \"dropout\": 0.3},\\n\\n    \"graph\": {\\n        \"atom_embed_dim\": 64,\\n        \"edge_feat_dim\": 4,\\n        \"radius\": 5.0,\\n        \"max_neighbors\": 24\\n    }\\n}\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Numeric feature columns\n",
    "num_cols = [\n",
    "    \"formation_energy_peratom\", \"optb88vdw_bandgap\",\n",
    "    \"slme\", \"magmom_oszicar\", \"spillage\", \"kpoint_length_unit\",\n",
    "    \"optb88vdw_total_energy\", \"epsx\", \"epsy\", \"epsz\", \"density\",\n",
    "    \"poisson\", \"nat\", \"bulk_modulus_kv\", \"shear_modulus_gv\",\n",
    "    \"mbj_bandgap\", \"hse_gap\", \"ehull\", \"Tc_supercon\"\n",
    "]\n",
    "\n",
    "# Unified configuration\n",
    "config_model = {\n",
    "    \"devices\": {\"model\": \"cuda\"},\n",
    "    \"threshold\": 0.94,\n",
    "\n",
    "    \"numeric_cols\": num_cols,\n",
    "\n",
    "    \"categorical\": {\n",
    "        \"spg_number\": {\"vocab_size\": 214, \"embed_dim\": 50},\n",
    "        \"func\": {\"vocab_size\": 2, \"embed_dim\": 1},\n",
    "        \"dimensionality\": {\"vocab_size\": 8, \"embed_dim\": 4},\n",
    "        \"typ\": {\"vocab_size\": 2, \"embed_dim\": 1},\n",
    "        \"crys\": {\"vocab_size\": 8, \"embed_dim\": 4},\n",
    "    },\n",
    "\n",
    "    \"formula\": {\"num_elements\": 89, \"embed_dim\": 32},\n",
    "\n",
    "    \"mlp\": {\"hidden1\": 128, \"hidden2\": 64, \"dropout\": 0.3},\n",
    "\n",
    "    \"graph\": {\n",
    "        \"atom_embed_dim\": 64,\n",
    "        \"edge_feat_dim\": 4,\n",
    "        \"radius\": 5.0,\n",
    "        \"max_neighbors\": 24\n",
    "    }\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745fb1f7",
   "metadata": {},
   "source": [
    "## Candidate Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3378c2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_candidate_column(df: pd.DataFrame, config: dict) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    bandgap_col   = config[\"filters\"][\"bandgap_column\"]\n",
    "    sem_min       = config[\"filters\"][\"semiconductor_min\"]\n",
    "    sem_max       = config[\"filters\"][\"semiconductor_max\"]\n",
    "    trans_min     = config[\"filters\"][\"transparent_min\"]\n",
    "    toxic_elements = config[\"filters\"][\"toxic_elements\"]\n",
    "\n",
    "    df[bandgap_col] = pd.to_numeric(df[bandgap_col], errors=\"coerce\")\n",
    "    in_semiconductor_range = df[bandgap_col].between(sem_min, sem_max)\n",
    "    is_transparent = df[bandgap_col] > trans_min\n",
    "\n",
    "    if \"ehull\" in df.columns:\n",
    "        df[\"ehull\"] = pd.to_numeric(df[\"ehull\"], errors=\"coerce\")\n",
    "        is_stable = df[\"ehull\"] < config[\"filters\"][\"ehull\"]\n",
    "    else:\n",
    "        is_stable = True\n",
    "\n",
    "    if \"formula\" in df.columns:\n",
    "        tokens = df[\"formula\"].fillna(\"\").astype(str).str.findall(r\"[A-Z][a-z]?\")\n",
    "        has_toxic = tokens.apply(lambda t: any(el in t for el in toxic_elements))\n",
    "    else:\n",
    "        has_toxic = False\n",
    "\n",
    "    df[\"is_candidate\"] = (\n",
    "        in_semiconductor_range &\n",
    "        is_transparent &\n",
    "        is_stable &\n",
    "        (has_toxic == False)\n",
    "    ).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply candidate filter\n",
    "config_cand = {\n",
    "    \"filters\": {\n",
    "        \"bandgap_column\": \"optb88vdw_bandgap\",\n",
    "        \"semiconductor_min\": 0.5,\n",
    "        \"semiconductor_max\": 5.0,\n",
    "        \"transparent_min\": 2.5,\n",
    "        \"toxic_elements\": [\"Pb\", \"Cd\", \"As\", \"Hg\"],\n",
    "        \"ehull\": 0.1\n",
    "    }\n",
    "}\n",
    "\n",
    "df = add_candidate_column(df, config_cand)\n",
    "df = df.rename(columns={\"is_candidate\": \"target\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed67df59",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d5f6d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access numeric columns from the TOML config\n",
    "numeric_cols = config[\"model\"][\"num_cols\"]\n",
    "\n",
    "# Force numeric dtype, coerce errors to NaN\n",
    "df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# Optionally fill NaNs with 0 or column mean\n",
    "df[numeric_cols] = df[numeric_cols].fillna(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fae176",
   "metadata": {},
   "source": [
    "## Model Definition (Multimodal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "113e64bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multimodal model - numeric + categorical + formula + graph (LayerNorm fix)\n",
    "\n",
    "import re\n",
    "import ast\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from jarvis.core.atoms import Atoms\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers: element vocab & formula counts\n",
    "# ----------------------------\n",
    "def build_element_vocab(df, max_elements=89):\n",
    "    elems = set()\n",
    "    if \"formula\" in df.columns:\n",
    "        tokens = df[\"formula\"].fillna(\"\").astype(str).str.findall(r\"[A-Z][a-z]?\")\n",
    "        for t in tokens:\n",
    "            elems.update(t)\n",
    "    elems = sorted(list(elems))\n",
    "    vocab = {}\n",
    "    start_idx = 0\n",
    "    if len(elems) + 1 > max_elements:\n",
    "        vocab[\"UNK\"] = 0\n",
    "        start_idx = 1\n",
    "    for i, e in enumerate(elems[: max_elements - start_idx]):\n",
    "        vocab[e] = i + start_idx\n",
    "    return vocab\n",
    "\n",
    "def formula_to_counts(formula, element_vocab, num_elements):\n",
    "    counts = np.zeros(num_elements, dtype=np.float32)\n",
    "    if not isinstance(formula, str):\n",
    "        return counts\n",
    "    tokens = re.findall(r\"([A-Z][a-z]?)(\\d*\\.?\\d*)\", formula)\n",
    "    for sym, num in tokens:\n",
    "        idx = element_vocab.get(sym, element_vocab.get(\"UNK\", None))\n",
    "        if idx is None:\n",
    "            continue\n",
    "        val = float(num) if num not in (\"\", None) else 1.0\n",
    "        counts[idx] += val\n",
    "    return counts\n",
    "\n",
    "# ----------------------------\n",
    "# Graph building from atoms_obj\n",
    "# ----------------------------\n",
    "def atoms_to_graph(atoms_obj, radius=5.0, max_neighbors=None, element_vocab=None, num_elements=89, device=\"cpu\"):\n",
    "    cart_coords = np.array(atoms_obj.cart_coords)\n",
    "    species = atoms_obj.elements\n",
    "    N = len(species)\n",
    "\n",
    "    node_idx = np.array([element_vocab.get(sym, element_vocab.get(\"UNK\", 0)) for sym in species], dtype=np.int64)\n",
    "\n",
    "    edge_src, edge_dst, edge_attr = [], [], []\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i == j:\n",
    "                continue\n",
    "            rij = cart_coords[j] - cart_coords[i]\n",
    "            dist = np.linalg.norm(rij)\n",
    "            if dist <= radius:\n",
    "                inv = 1.0 / dist if dist > 1e-8 else 0.0\n",
    "                unit = rij / dist if dist > 1e-8 else np.zeros(3)\n",
    "                feat = [dist, inv, unit[0], unit[1]]  # edge_feat_dim=4\n",
    "                edge_src.append(i); edge_dst.append(j); edge_attr.append(feat)\n",
    "\n",
    "    if max_neighbors is not None and len(edge_src) > 0:\n",
    "        capped_src, capped_dst, capped_attr = [], [], []\n",
    "        edges_by_src = {}\n",
    "        for s, d, a in zip(edge_src, edge_dst, edge_attr):\n",
    "            edges_by_src.setdefault(s, []).append((d, a))\n",
    "        for s, lst in edges_by_src.items():\n",
    "            lst_sorted = sorted(lst, key=lambda x: x[1][0])[:max_neighbors]\n",
    "            for d, a in lst_sorted:\n",
    "                capped_src.append(s); capped_dst.append(d); capped_attr.append(a)\n",
    "        edge_src, edge_dst, edge_attr = capped_src, capped_dst, capped_attr\n",
    "\n",
    "    x = torch.tensor(node_idx, dtype=torch.long, device=device)\n",
    "    if len(edge_src) == 0:\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long, device=device)\n",
    "        edge_attr_t = torch.empty((0, 4), dtype=torch.float32, device=device)\n",
    "    else:\n",
    "        edge_index = torch.stack([\n",
    "            torch.tensor(edge_src, dtype=torch.long, device=device),\n",
    "            torch.tensor(edge_dst, dtype=torch.long, device=device)\n",
    "        ], dim=0)  # (2, E)\n",
    "        edge_attr_t = torch.tensor(edge_attr, dtype=torch.float32, device=device)  # (E, 4)\n",
    "\n",
    "    return {\"x\": x, \"edge_index\": edge_index, \"edge_attr\": edge_attr_t}\n",
    "\n",
    "# ----------------------------\n",
    "# Embedding modules\n",
    "# ----------------------------\n",
    "class FormulaEmbedder(nn.Module):\n",
    "    def __init__(self, num_elements, embed_dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_elements, embed_dim)\n",
    "\n",
    "    def forward(self, element_counts):\n",
    "        idx = torch.arange(element_counts.shape[-1], device=element_counts.device)\n",
    "        weights = element_counts.float().unsqueeze(-1)\n",
    "        vecs = self.emb(idx)\n",
    "        return (vecs * weights).sum(dim=0)\n",
    "\n",
    "class SimpleGraphEncoder(nn.Module):\n",
    "    def __init__(self, num_elements, atom_embed_dim, edge_feat_dim=4):\n",
    "        super().__init__()\n",
    "        self.atom_emb = nn.Embedding(num_elements, atom_embed_dim)\n",
    "        self.msg1 = nn.Linear(atom_embed_dim + edge_feat_dim, atom_embed_dim)\n",
    "        self.msg2 = nn.Linear(atom_embed_dim + edge_feat_dim, atom_embed_dim)\n",
    "        self.node_up1 = nn.Linear(atom_embed_dim, atom_embed_dim)\n",
    "        self.node_up2 = nn.Linear(atom_embed_dim, atom_embed_dim)\n",
    "\n",
    "    def _normalize_edge_index(self, edge_index):\n",
    "        if edge_index.numel() == 0:\n",
    "            return edge_index\n",
    "        if edge_index.ndim == 1:\n",
    "            E = edge_index.numel() // 2\n",
    "            edge_index = edge_index.view(2, E)\n",
    "        elif edge_index.ndim == 2:\n",
    "            if edge_index.shape[0] == 2:\n",
    "                pass\n",
    "            elif edge_index.shape[1] == 2:\n",
    "                edge_index = edge_index.t()\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected edge_index shape: {tuple(edge_index.shape)}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected edge_index ndim: {edge_index.ndim}\")\n",
    "        return edge_index\n",
    "\n",
    "    def message_pass(self, x, edge_index, edge_attr):\n",
    "        if edge_index.numel() == 0:\n",
    "            return x\n",
    "        edge_index = self._normalize_edge_index(edge_index)\n",
    "        src, dst = edge_index\n",
    "        E = edge_index.shape[1]\n",
    "        if edge_attr.ndim == 1:\n",
    "            edge_attr = edge_attr.view(E, -1)\n",
    "        elif edge_attr.ndim == 2 and edge_attr.shape[0] != E and edge_attr.shape[1] == E:\n",
    "            edge_attr = edge_attr.t()\n",
    "        elif edge_attr.ndim != 2 or edge_attr.shape[0] != E:\n",
    "            raise ValueError(f\"edge_attr shape mismatch: {tuple(edge_attr.shape)} vs E={E}\")\n",
    "        x_src = x[src]\n",
    "        m = torch.cat([x_src, edge_attr], dim=-1)\n",
    "        m = F.relu(self.msg1(m))\n",
    "        agg = torch.zeros_like(x)\n",
    "        agg.index_add_(0, dst, m)\n",
    "        x = F.relu(self.node_up1(x + agg))\n",
    "        x_src = x[src]\n",
    "        m = torch.cat([x_src, edge_attr], dim=-1)\n",
    "        m = F.relu(self.msg2(m))\n",
    "        agg = torch.zeros_like(x)\n",
    "        agg.index_add_(0, dst, m)\n",
    "        x = F.relu(self.node_up2(x + agg))\n",
    "        return x\n",
    "\n",
    "    def forward(self, graph):\n",
    "        x = self.atom_emb(graph[\"x\"])\n",
    "        x = self.message_pass(x, graph[\"edge_index\"], graph[\"edge_attr\"])\n",
    "        return x.mean(dim=0) if x.shape[0] > 0 else torch.zeros(self.atom_emb.embedding_dim, device=x.device)\n",
    "\n",
    "class CategoricalEmbeddings(nn.Module):\n",
    "    def __init__(self, cat_cfg):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleDict({\n",
    "            name: nn.Embedding(params[\"vocab_size\"], params[\"embed_dim\"])\n",
    "            for name, params in cat_cfg.items()\n",
    "        })\n",
    "\n",
    "    def forward(self, X_cat_dict):\n",
    "        embs = [self.embeddings[name](X_cat_dict[name]) for name in self.embeddings]\n",
    "        return torch.cat([e if e.dim() > 1 else e.unsqueeze(0) for e in embs], dim=-1)\n",
    "\n",
    "# ----------------------------\n",
    "# Full multimodal model (LayerNorm)\n",
    "# ----------------------------\n",
    "class CandidateNetMultimodal(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.cfg = config\n",
    "\n",
    "        self.num_proj = nn.Identity()\n",
    "        self.cat_emb = CategoricalEmbeddings(config[\"categorical\"])\n",
    "        cat_total_dim = sum([p[\"embed_dim\"] for p in config[\"categorical\"].values()])\n",
    "\n",
    "        self.formula_emb = FormulaEmbedder(config[\"formula\"][\"num_elements\"], config[\"formula\"][\"embed_dim\"])\n",
    "        self.graph_enc = SimpleGraphEncoder(config[\"formula\"][\"num_elements\"], config[\"graph\"][\"atom_embed_dim\"], config[\"graph\"][\"edge_feat_dim\"])\n",
    "\n",
    "        in_dim = len(config[\"num_cols\"]) + cat_total_dim + config[\"formula\"][\"embed_dim\"] + config[\"graph\"][\"atom_embed_dim\"]\n",
    "\n",
    "        # Replace BatchNorm1d with LayerNorm to support batch_size=1\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_dim, config[\"mlp\"][\"hidden1\"]),\n",
    "            nn.LayerNorm(config[\"mlp\"][\"hidden1\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=config[\"mlp\"][\"dropout\"]),\n",
    "            nn.Linear(config[\"mlp\"][\"hidden1\"], config[\"mlp\"][\"hidden2\"]),\n",
    "            nn.LayerNorm(config[\"mlp\"][\"hidden2\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config[\"mlp\"][\"hidden2\"], 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, X_num, X_cat_dict, formula_counts, graph):\n",
    "        x_num = self.num_proj(X_num.float())\n",
    "        x_cat = self.cat_emb(X_cat_dict)\n",
    "\n",
    "        if formula_counts.dim() == 2:\n",
    "            x_formula = torch.stack([self.formula_emb(fc) for fc in formula_counts])\n",
    "        else:\n",
    "            x_formula = self.formula_emb(formula_counts).unsqueeze(0)\n",
    "\n",
    "        x_graph = self.graph_enc(graph)\n",
    "        if x_graph.dim() == 1:\n",
    "            x_graph = x_graph.unsqueeze(0)\n",
    "\n",
    "        x = torch.cat([x_num, x_cat, x_formula, x_graph], dim=-1)\n",
    "        return self.fc(x).squeeze(-1)\n",
    "\n",
    "    def predict_proba(self, *args, **kwargs):\n",
    "        logits = self.forward(*args, **kwargs)\n",
    "        return torch.sigmoid(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bdf2c7",
   "metadata": {},
   "source": [
    "## Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550c9bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:  40%|████      | 766/1900 [00:36<00:52, 21.64it/s]"
     ]
    }
   ],
   "source": [
    "# Training & evaluation - dataset prep, loaders, loop, metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use TOML config\n",
    "model_cfg = config[\"model\"]\n",
    "\n",
    "device = torch.device(model_cfg[\"devices\"][\"model\"] if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----------------------------\n",
    "# Prepare categorical indices\n",
    "# ----------------------------\n",
    "cat_cols = list(model_cfg[\"categorical\"].keys())\n",
    "cat_maps = {}\n",
    "for col in cat_cols:\n",
    "    codes, uniques = pd.factorize(df[col].fillna(\"UNK\").astype(str))\n",
    "    df[f\"{col}_idx\"] = codes\n",
    "    cat_maps[col] = {u: i for i, u in enumerate(uniques)}\n",
    "\n",
    "# ----------------------------\n",
    "# Prepare atoms_obj column (from df['atoms'])\n",
    "# ----------------------------\n",
    "def ensure_atoms_obj(df):\n",
    "    def to_atoms(x):\n",
    "        try:\n",
    "            d = ast.literal_eval(x) if isinstance(x, str) else x\n",
    "            return Atoms.from_dict(d)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return df[\"atoms\"].apply(to_atoms)\n",
    "\n",
    "if \"atoms_obj\" not in df.columns and \"atoms\" in df.columns:\n",
    "    df[\"atoms_obj\"] = ensure_atoms_obj(df)\n",
    "else:\n",
    "    df[\"atoms_obj\"] = df[\"atoms_obj\"] if \"atoms_obj\" in df.columns else None\n",
    "\n",
    "# ----------------------------\n",
    "# Element vocab and formula counts\n",
    "# ----------------------------\n",
    "element_vocab = build_element_vocab(df, max_elements=model_cfg[\"formula\"][\"num_elements\"])\n",
    "num_elements = model_cfg[\"formula\"][\"num_elements\"]\n",
    "df[\"formula_counts\"] = df[\"formula\"].apply(lambda f: formula_to_counts(f, element_vocab, num_elements))\n",
    "\n",
    "# ----------------------------\n",
    "# Dataset class\n",
    "# ----------------------------\n",
    "class MaterialsDataset(Dataset):\n",
    "    def __init__(self, frame, config):\n",
    "        self.df = frame.reset_index(drop=True)\n",
    "        self.cfg = config\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        X_num = torch.tensor(row[self.cfg[\"num_cols\"]].astype(float).values,\n",
    "                             dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        X_cat_dict = {\n",
    "            name: torch.tensor([row[f\"{name}_idx\"]], dtype=torch.long, device=device)\n",
    "            for name in self.cfg[\"categorical\"].keys()\n",
    "        }\n",
    "\n",
    "        counts = torch.tensor(row[\"formula_counts\"], dtype=torch.float32, device=device)\n",
    "\n",
    "        aobj = row.get(\"atoms_obj\", None)\n",
    "        if aobj is None:\n",
    "            graph = {\n",
    "                \"x\": torch.empty((0,), dtype=torch.long, device=device),\n",
    "                \"edge_index\": torch.empty((2, 0), dtype=torch.long, device=device),\n",
    "                \"edge_attr\": torch.empty((0, self.cfg[\"graph\"][\"edge_feat_dim\"]),\n",
    "                                         dtype=torch.float32, device=device)\n",
    "            }\n",
    "        else:\n",
    "            graph = atoms_to_graph(\n",
    "                aobj,\n",
    "                radius=self.cfg[\"graph\"][\"radius\"],\n",
    "                max_neighbors=self.cfg[\"graph\"][\"max_neighbors\"],\n",
    "                element_vocab=element_vocab,\n",
    "                num_elements=num_elements,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "        y = torch.tensor([row[\"target\"]], dtype=torch.float32, device=device)\n",
    "\n",
    "        return X_num, X_cat_dict, counts, graph, y\n",
    "\n",
    "# ----------------------------\n",
    "# Custom collate to keep graph dict intact\n",
    "# ----------------------------\n",
    "def collate_single(batch):\n",
    "    X_num, X_cat_dict, counts, graph, y = batch[0]\n",
    "    return X_num, X_cat_dict, counts, graph, y\n",
    "\n",
    "# ----------------------------\n",
    "# Train/val split and loaders\n",
    "# ----------------------------\n",
    "train_df, val_df = train_test_split(df, test_size=0.2,\n",
    "                                    stratify=df[\"target\"], random_state=42)\n",
    "train_ds = MaterialsDataset(train_df, model_cfg)\n",
    "val_ds   = MaterialsDataset(val_df, model_cfg)\n",
    "\n",
    "batch_size = model_cfg.get(\"training\", {}).get(\"batch_size\", 32)\n",
    "\n",
    "train_loader = DataLoader(train_ds,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=collate_single)\n",
    "\n",
    "val_loader   = DataLoader(val_ds,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=collate_single)\n",
    "\n",
    "# ----------------------------\n",
    "# Model, loss, optimizer\n",
    "# ----------------------------\n",
    "model = CandidateNetMultimodal(model_cfg).to(device)\n",
    "\n",
    "pos = int((train_df[\"target\"] == 1).sum())\n",
    "neg = int((train_df[\"target\"] == 0).sum())\n",
    "pos_weight = torch.tensor([neg / max(pos, 1)], device=device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# ----------------------------\n",
    "# Training loop with progress bar + logging\n",
    "# ----------------------------\n",
    "epochs = model_cfg.get(\"training\", {}).get(\"epochs\", 10)\n",
    "history = {\"loss\": [], \"val_acc\": [], \"val_prec\": [], \"val_rec\": [], \"val_f1\": [], \"val_auc\": []}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for X_num, X_cat_dict, counts, graph, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X_num, X_cat_dict, counts, graph)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    history[\"loss\"].append(avg_loss)\n",
    "    logger.info(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "    flush_logger(logger)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Evaluation\n",
    "    # ----------------------------\n",
    "    model.eval()\n",
    "    y_true, y_prob = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_num, X_cat_dict, counts, graph, y in val_loader:\n",
    "            logits = model(X_num, X_cat_dict, counts, graph)\n",
    "            prob = torch.sigmoid(logits).item()\n",
    "            y_prob.append(prob)\n",
    "            y_true.append(y.item())\n",
    "\n",
    "    threshold = model_cfg[\"threshold\"]\n",
    "    y_pred = (np.array(y_prob) >= threshold).astype(int)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred,\n",
    "                                                       average=\"binary\", zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "    except Exception:\n",
    "        auc = float(\"nan\")\n",
    "\n",
    "    history[\"val_acc\"].append(acc)\n",
    "    history[\"val_prec\"].append(prec)\n",
    "    history[\"val_rec\"].append(rec)\n",
    "    history[\"val_f1\"].append(f1)\n",
    "    history[\"val_auc\"].append(auc)\n",
    "\n",
    "    logger.info(f\"Validation -- Acc: {acc:.3f} | Prec: {prec:.3f} | Rec: {rec:.3f} | F1: {f1:.3f} | AUC: {auc:.3f}\")\n",
    "    flush_logger(logger)\n",
    "\n",
    "# ----------------------------\n",
    "# Plot training history\n",
    "# ----------------------------\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(history[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(history[\"val_acc\"], label=\"Val Accuracy\")\n",
    "plt.plot(history[\"val_f1\"], label=\"Val F1\")\n",
    "plt.plot(history[\"val_auc\"], label=\"Val AUC\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Metric\")\n",
    "plt.legend()\n",
    "plt.title(\"Training History\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea4fdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: torch.Size([1]) y shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(\"logits shape:\", logits.shape, \"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a73fe6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CandidateNetMultimodal(\n",
      "  (num_proj): Identity()\n",
      "  (cat_emb): CategoricalEmbeddings(\n",
      "    (embeddings): ModuleDict(\n",
      "      (spg_number): Embedding(214, 50)\n",
      "      (func): Embedding(2, 1)\n",
      "      (dimensionality): Embedding(8, 4)\n",
      "      (typ): Embedding(2, 1)\n",
      "      (crys): Embedding(8, 4)\n",
      "    )\n",
      "  )\n",
      "  (formula_emb): FormulaEmbedder(\n",
      "    (emb): Embedding(89, 32)\n",
      "  )\n",
      "  (graph_enc): SimpleGraphEncoder(\n",
      "    (atom_emb): Embedding(89, 64)\n",
      "    (msg1): Linear(in_features=68, out_features=64, bias=True)\n",
      "    (msg2): Linear(in_features=68, out_features=64, bias=True)\n",
      "    (node_up1): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (node_up2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=175, out_features=128, bias=True)\n",
      "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b421571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
