{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a60d09a",
   "metadata": {},
   "source": [
    "# GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbbdc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import io\n",
    "import sys\n",
    "import toml\n",
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from jarvis.db.figshare import data\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9d9bca",
   "metadata": {},
   "source": [
    "## Get Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f06999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Configuration:\n",
      "{'data': {'dataset_name': 'dft_3d', 'store_dir': '/shared/data/jarvis'},\n",
      " 'features': {'bag_of_elements': True,\n",
      "              'derived': ['eps_mean', 'eps_std'],\n",
      "              'use_columns': ['ehull',\n",
      "                              'formation_energy_peratom',\n",
      "                              'avg_elec_mass',\n",
      "                              'avg_hole_mass',\n",
      "                              'effective_masses_297K',\n",
      "                              'epsx',\n",
      "                              'epsy',\n",
      "                              'epsz',\n",
      "                              'natoms']},\n",
      " 'filters': {'bandgap_column': 'optb88vdw_bandgap',\n",
      "             'max_eps': 10.0,\n",
      "             'min_eps': 1.0,\n",
      "             'semiconductor_max': 4.0,\n",
      "             'semiconductor_min': 0.5,\n",
      "             'toxic_elements': ['Pb', 'Cd', 'Hg', 'As', 'Se'],\n",
      "             'transparent_min': 3.0},\n",
      " 'known': {'transparent_formulas': ['In2O3',\n",
      "                                    'ZnO',\n",
      "                                    'SnO2',\n",
      "                                    'Ga2O3',\n",
      "                                    'TiO2',\n",
      "                                    'SrTiO3',\n",
      "                                    'BaSnO3',\n",
      "                                    'SrVO3',\n",
      "                                    'Al2O3',\n",
      "                                    'SiO2',\n",
      "                                    'MgO',\n",
      "                                    'GaN',\n",
      "                                    'SiC']},\n",
      " 'logging': {'enable': True,\n",
      "             'file': '/shared/data/jarvis/project.log',\n",
      "             'level': 'INFO'},\n",
      " 'ml': {'exclude_columns': ['optb88vdw_bandgap',\n",
      "                            'mbj_bandgap',\n",
      "                            'hse_gap',\n",
      "                            'formula',\n",
      "                            'jid'],\n",
      "        'model': 'logreg',\n",
      "        'target_column': 'is_transparent'},\n",
      " 'project': {'author': 'Brent Allen Thorne',\n",
      "             'name': 'Transparent Nontoxic Semiconductors',\n",
      "             'version': '1.0'},\n",
      " 'system': {'path': './'}}\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CONFIG_PATH = \"config.toml\"\n",
    "config = toml.load(CONFIG_PATH)\n",
    "\n",
    "# Pretty print configuration\n",
    "print(\"Project Configuration:\")\n",
    "pprint.pprint(config)\n",
    "\n",
    "# Set up system path\n",
    "SYS_PATH = config.get('system', {}).get('path', './')\n",
    "sys.path.append(SYS_PATH) # .../code/jarvis/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f50698",
   "metadata": {},
   "source": [
    "## Setup Logger and Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af423f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 21:16:28,184 - jarvis_project - INFO - Project configuration loaded.\n",
      "2025-12-03 21:16:28,184 - jarvis_project - INFO - Dataset: dft_3d\n",
      "2025-12-03 21:16:28,185 - jarvis_project - INFO - Store directory: /shared/data/jarvis\n",
      "2025-12-03 21:16:29,391 - jarvis_project - INFO - Dataset shape: (75993, 64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (75993, 64)\n"
     ]
    }
   ],
   "source": [
    "# Custom Imports and Configurations\n",
    "from jarvis_utils import load_or_fetch_dataset\n",
    "from logger_utils import setup_logger, flush_logger\n",
    "from filter_utils import apply_filters\n",
    "from featurizer import Featurizer\n",
    "\n",
    "# Setup logger\n",
    "logger = setup_logger(config)\n",
    "\n",
    "logger.info(\"Project configuration loaded.\")\n",
    "logger.info(f\"Dataset: {config['data']['dataset_name']}\")\n",
    "logger.info(f\"Store directory: {config['data']['store_dir']}\")\n",
    "\n",
    "# Load dataset\n",
    "df = load_or_fetch_dataset(config[\"data\"][\"dataset_name\"], data, config[\"data\"][\"store_dir\"])\n",
    "logger.info(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74a049b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 21:16:29,400 - jarvis_project - INFO - Features: ['jid', 'spg_number', 'spg_symbol', 'formula', 'formation_energy_peratom', 'func', 'optb88vdw_bandgap', 'atoms', 'slme', 'magmom_oszicar', 'spillage', 'elastic_tensor', 'effective_masses_300K', 'kpoint_length_unit', 'maxdiff_mesh', 'maxdiff_bz', 'encut', 'optb88vdw_total_energy', 'epsx', 'epsy', 'epsz', 'mepsx', 'mepsy', 'mepsz', 'modes', 'magmom_outcar', 'max_efg', 'avg_elec_mass', 'avg_hole_mass', 'icsd', 'dfpt_piezo_max_eij', 'dfpt_piezo_max_dij', 'dfpt_piezo_max_dielectric', 'dfpt_piezo_max_dielectric_electronic', 'dfpt_piezo_max_dielectric_ionic', 'max_ir_mode', 'min_ir_mode', 'n-Seebeck', 'p-Seebeck', 'n-powerfact', 'p-powerfact', 'ncond', 'pcond', 'nkappa', 'pkappa', 'ehull', 'Tc_supercon', 'dimensionality', 'efg', 'xml_data_link', 'typ', 'exfoliation_energy', 'spg', 'crys', 'density', 'poisson', 'raw_files', 'nat', 'bulk_modulus_kv', 'shear_modulus_gv', 'mbj_bandgap', 'hse_gap', 'reference', 'search']\n"
     ]
    }
   ],
   "source": [
    "features = df.columns.tolist()\n",
    "logger.info(f\"Features: {features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46898e",
   "metadata": {},
   "source": [
    "## Configuration and feature schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f36ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\n",
    "        \"formation_energy_peratom\", \"optb88vdw_bandgap\",\n",
    "        \"slme\", \"magmom_oszicar\", \"spillage\", \"kpoint_length_unit\",\n",
    "        \"optb88vdw_total_energy\", \"epsx\", \"epsy\", \"epsz\", \"density\",\n",
    "        \"poisson\", \"nat\", \"bulk_modulus_kv\", \"shear_modulus_gv\",\n",
    "        \"mbj_bandgap\", \"hse_gap\", \"ehull\", \"Tc_supercon\"\n",
    "        # add other numeric features as needed\n",
    "    ]\n",
    "\n",
    "# Configuration for feature dimensions and embeddings\n",
    "config = {\n",
    "    \"devices\": {\"model\": \"cuda\"},\n",
    "    \"threshold\": 0.94,  # deployment cutoff, if you want to keep it here\n",
    "\n",
    "    \"numeric_cols\": num_cols,  # list of numeric feature column names\n",
    "\n",
    "    # Categorical vocab sizes (from our dataset preprocessing)\n",
    "    \"categorical\": {\n",
    "        \"spg_number\": {\"vocab_size\": 214, \"embed_dim\": 50},\n",
    "        \"func\": {\"vocab_size\": 2, \"embed_dim\": 1},\n",
    "        \"dimensionality\": {\"vocab_size\": 8, \"embed_dim\": 4},\n",
    "        \"typ\": {\"vocab_size\": 2, \"embed_dim\": 1},\n",
    "        \"crys\": {\"vocab_size\": 8, \"embed_dim\": 4},\n",
    "    },\n",
    "\n",
    "    # Formula embedding\n",
    "    \"formula\": {\"num_elements\": 89, \"embed_dim\": 32},\n",
    "\n",
    "    # MLP head sizes\n",
    "    \"mlp\": {\"hidden1\": 128, \"hidden2\": 64, \"dropout\": 0.3},\n",
    "\n",
    "    # Graph settings\n",
    "    \"graph\": {\n",
    "        \"atom_embed_dim\": 64,   # node embedding size\n",
    "        \"edge_feat_dim\": 4,     # e.g., distance, 1/r, angle or dummy padding\n",
    "        \"radius\": 5.0,          # neighbor cutoff (Å)\n",
    "        \"max_neighbors\": 24     # cap neighbors per atom (optional)\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745fb1f7",
   "metadata": {},
   "source": [
    "## Add Candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3378c2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_candidate_column(df: pd.DataFrame, config: dict) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    bandgap_col   = config[\"filters\"][\"bandgap_column\"]\n",
    "    sem_min       = config[\"filters\"][\"semiconductor_min\"]\n",
    "    sem_max       = config[\"filters\"][\"semiconductor_max\"]\n",
    "    trans_min     = config[\"filters\"][\"transparent_min\"]\n",
    "    toxic_elements = config[\"filters\"][\"toxic_elements\"]\n",
    "\n",
    "    df[bandgap_col] = pd.to_numeric(df[bandgap_col], errors=\"coerce\")\n",
    "    in_semiconductor_range = df[bandgap_col].between(sem_min, sem_max)\n",
    "    is_transparent = df[bandgap_col] > trans_min\n",
    "\n",
    "    if \"ehull\" in df.columns:\n",
    "        df[\"ehull\"] = pd.to_numeric(df[\"ehull\"], errors=\"coerce\")\n",
    "        is_stable = df[\"ehull\"] < 0.1\n",
    "    else:\n",
    "        is_stable = True\n",
    "\n",
    "    if \"formula\" in df.columns:\n",
    "        tokens = df[\"formula\"].fillna(\"\").astype(str).str.findall(r\"[A-Z][a-z]?\")\n",
    "        has_toxic = tokens.apply(lambda t: any(el in t for el in toxic_elements))\n",
    "    else:\n",
    "        has_toxic = False\n",
    "\n",
    "    df[\"is_candidate\"] = (\n",
    "        in_semiconductor_range &\n",
    "        is_transparent &\n",
    "        is_stable &\n",
    "        (has_toxic == False)\n",
    "    ).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "config_cand = {\n",
    "    \"filters\": {\n",
    "        \"bandgap_column\": \"optb88vdw_bandgap\",\n",
    "        \"semiconductor_min\": 0.5,\n",
    "        \"semiconductor_max\": 5.0,\n",
    "        \"transparent_min\": 2.5,\n",
    "        \"toxic_elements\": [\"Pb\", \"Cd\", \"As\", \"Hg\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "df = add_candidate_column(df, config_cand)\n",
    "df = df.rename(columns={\"is_candidate\": \"target\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fae176",
   "metadata": {},
   "source": [
    "## Build atomic graphs from atoms_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113e64bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FormulaEmbedder(nn.Module):\n",
    "    def __init__(self, num_elements, embed_dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_elements, embed_dim)\n",
    "\n",
    "    def forward(self, element_counts):  # dict-like or tensor of counts\n",
    "        # element_counts: tensor [num_elements] with counts for the formula\n",
    "        # Create indices for elements present\n",
    "        idx = torch.arange(element_counts.shape[-1], device=element_counts.device)\n",
    "        weights = element_counts.float().unsqueeze(-1)       # [E,1]\n",
    "        vecs = self.emb(idx)                                 # [E, D]\n",
    "        # Weighted sum of element embeddings (composition-level representation)\n",
    "        return (vecs * weights).sum(dim=0)                   # [D]\n",
    "\n",
    "\n",
    "class SimpleGraphEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight message-passing encoder:\n",
    "    - node embedding from element indices\n",
    "    - edge conditioning from distances/unit vectors\n",
    "    - 2-layer message passing + global pooling\n",
    "    \"\"\"\n",
    "    def __init__(self, num_elements, atom_embed_dim, edge_feat_dim):\n",
    "        super().__init__()\n",
    "        self.atom_emb = nn.Embedding(num_elements, atom_embed_dim)\n",
    "\n",
    "        self.msg1 = nn.Linear(atom_embed_dim + edge_feat_dim, atom_embed_dim)\n",
    "        self.msg2 = nn.Linear(atom_embed_dim + edge_feat_dim, atom_embed_dim)\n",
    "        self.node_up1 = nn.Linear(atom_embed_dim, atom_embed_dim)\n",
    "        self.node_up2 = nn.Linear(atom_embed_dim, atom_embed_dim)\n",
    "\n",
    "    def message_pass(self, x, edge_index, edge_attr):\n",
    "        if edge_index.numel() == 0:\n",
    "            return x\n",
    "        src, dst = edge_index\n",
    "        x_src = x[src]                             # [E, D]\n",
    "        m = torch.cat([x_src, edge_attr], dim=-1)  # [E, D+F]\n",
    "        m = F.relu(self.msg1(m))\n",
    "        # Aggregate messages per destination node (sum)\n",
    "        agg = torch.zeros_like(x)\n",
    "        agg.index_add_(0, dst, m)\n",
    "        x = F.relu(self.node_up1(x + agg))\n",
    "        # second round\n",
    "        x_src = x[src]\n",
    "        m = torch.cat([x_src, edge_attr], dim=-1)\n",
    "        m = F.relu(self.msg2(m))\n",
    "        agg = torch.zeros_like(x)\n",
    "        agg.index_add_(0, dst, m)\n",
    "        x = F.relu(self.node_up2(x + agg))\n",
    "        return x\n",
    "\n",
    "    def forward(self, graph):\n",
    "        # graph: dict with keys x (indices), edge_index, edge_attr\n",
    "        x = self.atom_emb(graph[\"x\"])                 # [N, D]\n",
    "        x = self.message_pass(x, graph[\"edge_index\"], graph[\"edge_attr\"])\n",
    "        # global pooling (mean)\n",
    "        if x.shape[0] == 0:\n",
    "            return torch.zeros(self.atom_emb.embedding_dim, device=x.device)\n",
    "        return x.mean(dim=0)                          # [D]\n",
    "\n",
    "\n",
    "class CategoricalEmbeddings(nn.Module):\n",
    "    def __init__(self, cat_cfg):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleDict({\n",
    "            name: nn.Embedding(params[\"vocab_size\"], params[\"embed_dim\"])\n",
    "            for name, params in cat_cfg.items()\n",
    "        })\n",
    "\n",
    "    def forward(self, X_cat_dict):\n",
    "        # X_cat_dict: dict of tensors (Long) per categorical feature\n",
    "        embs = [self.embeddings[name](X_cat_dict[name]) for name in self.embeddings]\n",
    "        # If inputs are batch-sized, concatenate along last dim\n",
    "        return torch.cat([e if e.dim() > 1 else e.unsqueeze(0) for e in embs], dim=-1)\n",
    "\n",
    "\n",
    "class CandidateNetMultimodal(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.cfg = config\n",
    "\n",
    "        # Numeric projection (optional normalization outside)\n",
    "        self.num_proj = nn.Identity()\n",
    "\n",
    "        # Categorical embeddings\n",
    "        self.cat_emb = CategoricalEmbeddings(config[\"categorical\"])\n",
    "        cat_total_dim = sum([p[\"embed_dim\"] for p in config[\"categorical\"].values()])\n",
    "\n",
    "        # Formula embedding\n",
    "        self.formula_emb = FormulaEmbedder(\n",
    "            num_elements=config[\"formula\"][\"num_elements\"],\n",
    "            embed_dim=config[\"formula\"][\"embed_dim\"]\n",
    "        )\n",
    "\n",
    "        # Graph encoder\n",
    "        self.graph_enc = SimpleGraphEncoder(\n",
    "            num_elements=config[\"formula\"][\"num_elements\"],\n",
    "            atom_embed_dim=config[\"graph\"][\"atom_embed_dim\"],\n",
    "            edge_feat_dim=config[\"graph\"][\"edge_feat_dim\"]\n",
    "        )\n",
    "\n",
    "        # Fusion MLP\n",
    "        in_dim = len(config[\"numeric_cols\"]) + cat_total_dim + config[\"formula\"][\"embed_dim\"] + config[\"graph\"][\"atom_embed_dim\"]\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_dim, config[\"mlp\"][\"hidden1\"]),\n",
    "            nn.BatchNorm1d(config[\"mlp\"][\"hidden1\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=config[\"mlp\"][\"dropout\"]),\n",
    "            nn.Linear(config[\"mlp\"][\"hidden1\"], config[\"mlp\"][\"hidden2\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config[\"mlp\"][\"hidden2\"], 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, X_num, X_cat_dict, formula_counts, graph):\n",
    "        # Numeric\n",
    "        x_num = self.num_proj(X_num.float())  # [B, num_numeric] or [num_numeric]\n",
    "\n",
    "        # Categorical\n",
    "        x_cat = self.cat_emb(X_cat_dict)      # [B, cat_dim] or [cat_dim]\n",
    "\n",
    "        # Formula (composition-level; if batch, apply per-sample)\n",
    "        if formula_counts.dim() == 2:  # [B, E]\n",
    "            x_formula = torch.stack([self.formula_emb(fc) for fc in formula_counts])\n",
    "        else:  # [E]\n",
    "            x_formula = self.formula_emb(formula_counts).unsqueeze(0)\n",
    "\n",
    "        # Graph (per-sample; if batching graphs, you’d pool per sample)\n",
    "        x_graph = self.graph_enc(graph)        # [D_graph]\n",
    "        if x_graph.dim() == 1:\n",
    "            x_graph = x_graph.unsqueeze(0)\n",
    "\n",
    "        # Concatenate\n",
    "        x = torch.cat([x_num, x_cat, x_formula, x_graph], dim=-1)\n",
    "\n",
    "        # Logits\n",
    "        return self.fc(x).squeeze(-1)          # [B]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee0981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_formula(formula):\n",
    "    pattern = r\"([A-Z][a-z]?)(\\d*)\"\n",
    "    matches = re.findall(pattern, formula)\n",
    "    comp = {}\n",
    "    for elem, count in matches:\n",
    "        comp[elem] = comp.get(elem, 0) + (int(count) if count else 1)\n",
    "    return comp\n",
    "\n",
    "\n",
    "# Build element vocabulary from the whole DataFrame\n",
    "all_elements = set()\n",
    "for f in df[\"formula\"]:\n",
    "    comp = parse_formula(f)\n",
    "    all_elements.update(comp.keys())\n",
    "\n",
    "element_vocab = {el: i for i, el in enumerate(sorted(all_elements))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39998f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Simple element-to-index mapping for formula embedding\n",
    "# (Assuming element_vocab is a dict {symbol: idx})\n",
    "element_vocab_inv = {v: k for k, v in element_vocab.items()}  # if needed\n",
    "num_elements = config[\"formula\"][\"num_elements\"]\n",
    "\n",
    "def element_symbol_to_index(symbol: str, vocab: dict) -> int:\n",
    "    return vocab.get(symbol, vocab.get(\"UNK\", 0))  # handle unknowns\n",
    "\n",
    "def atoms_to_graph(atoms_obj, radius=5.0, max_neighbors=None, element_vocab=None, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Build a graph from atoms_obj:\n",
    "    - nodes: element indices -> embedded later\n",
    "    - edges: undirected edges for neighbors within cutoff\n",
    "    - edge_attr: simple geometric features (distance, 1/dist, normalized distance components)\n",
    "    Returns dict with tensors suitable for batching.\n",
    "    \"\"\"\n",
    "    # Fractional -> Cartesian using lattice\n",
    "    cart_coords = np.array(atoms_obj.cart_coords)  # (N,3)\n",
    "    species = atoms_obj.elements  # list of symbols length N\n",
    "    N = len(species)\n",
    "\n",
    "    # Node features: element indices\n",
    "    node_idx = np.array([element_symbol_to_index(sym, element_vocab) for sym in species], dtype=np.int64)\n",
    "\n",
    "    # Build neighbor edges (naive O(N^2) cutoff; replace with spatial search if needed)\n",
    "    edge_src, edge_dst, edge_attr = [], [], []\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i == j:\n",
    "                continue\n",
    "            rij = cart_coords[j] - cart_coords[i]\n",
    "            dist = np.linalg.norm(rij)\n",
    "            if dist <= radius:\n",
    "                edge_src.append(i)\n",
    "                edge_dst.append(j)\n",
    "                # Edge features: [dist, 1/dist, dx/dist, dy/dist] (pad to edge_feat_dim)\n",
    "                inv = 1.0 / dist if dist > 1e-8 else 0.0\n",
    "                unit = rij / dist if dist > 1e-8 else np.zeros(3)\n",
    "                feat = [dist, inv, unit[0], unit[1]]\n",
    "                edge_attr.append(feat)\n",
    "\n",
    "    # Optional: cap neighbors per node\n",
    "    if max_neighbors is not None and len(edge_src) > 0:\n",
    "        # Simple capping by sorting edges per src by distance\n",
    "        capped_src, capped_dst, capped_attr = [], [], []\n",
    "        edges_by_src = {}\n",
    "        for s, d, a in zip(edge_src, edge_dst, edge_attr):\n",
    "            edges_by_src.setdefault(s, []).append((d, a))\n",
    "        for s, lst in edges_by_src.items():\n",
    "            # sort by distance (edge_attr[0])\n",
    "            lst_sorted = sorted(lst, key=lambda x: x[1][0])[:max_neighbors]\n",
    "            for d, a in lst_sorted:\n",
    "                capped_src.append(s)\n",
    "                capped_dst.append(d)\n",
    "                capped_attr.append(a)\n",
    "        edge_src, edge_dst, edge_attr = capped_src, capped_dst, capped_attr\n",
    "\n",
    "    # Convert to tensors\n",
    "    node_idx = torch.tensor(node_idx, dtype=torch.long, device=device)           # (N,)\n",
    "    x = node_idx  # node \"features\" are just indices; embedded later\n",
    "\n",
    "    if len(edge_src) == 0:\n",
    "        # Handle isolated atoms (rare for realistic crystals)\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long, device=device)\n",
    "        edge_attr = torch.empty((0, config[\"graph\"][\"edge_feat_dim\"]), dtype=torch.float32, device=device)\n",
    "    else:\n",
    "        edge_index = torch.tensor([edge_src, edge_dst], dtype=torch.long, device=device)  # (2, E)\n",
    "        edge_attr = torch.tensor(edge_attr, dtype=torch.float32, device=device)           # (E, F)\n",
    "\n",
    "    return {\"x\": x, \"edge_index\": edge_index, \"edge_attr\": edge_attr}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4351e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FormulaEmbedder(nn.Module):\n",
    "    def __init__(self, num_elements, embed_dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_elements, embed_dim)\n",
    "\n",
    "    def forward(self, element_counts):  # dict-like or tensor of counts\n",
    "        # element_counts: tensor [num_elements] with counts for the formula\n",
    "        # Create indices for elements present\n",
    "        idx = torch.arange(element_counts.shape[-1], device=element_counts.device)\n",
    "        weights = element_counts.float().unsqueeze(-1)       # [E,1]\n",
    "        vecs = self.emb(idx)                                 # [E, D]\n",
    "        # Weighted sum of element embeddings (composition-level representation)\n",
    "        return (vecs * weights).sum(dim=0)                   # [D]\n",
    "\n",
    "\n",
    "class SimpleGraphEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight message-passing encoder:\n",
    "    - node embedding from element indices\n",
    "    - edge conditioning from distances/unit vectors\n",
    "    - 2-layer message passing + global pooling\n",
    "    \"\"\"\n",
    "    def __init__(self, num_elements, atom_embed_dim, edge_feat_dim):\n",
    "        super().__init__()\n",
    "        self.atom_emb = nn.Embedding(num_elements, atom_embed_dim)\n",
    "\n",
    "        self.msg1 = nn.Linear(atom_embed_dim + edge_feat_dim, atom_embed_dim)\n",
    "        self.msg2 = nn.Linear(atom_embed_dim + edge_feat_dim, atom_embed_dim)\n",
    "        self.node_up1 = nn.Linear(atom_embed_dim, atom_embed_dim)\n",
    "        self.node_up2 = nn.Linear(atom_embed_dim, atom_embed_dim)\n",
    "\n",
    "    def message_pass(self, x, edge_index, edge_attr):\n",
    "        if edge_index.numel() == 0:\n",
    "            return x\n",
    "        src, dst = edge_index\n",
    "        x_src = x[src]                             # [E, D]\n",
    "        m = torch.cat([x_src, edge_attr], dim=-1)  # [E, D+F]\n",
    "        m = F.relu(self.msg1(m))\n",
    "        # Aggregate messages per destination node (sum)\n",
    "        agg = torch.zeros_like(x)\n",
    "        agg.index_add_(0, dst, m)\n",
    "        x = F.relu(self.node_up1(x + agg))\n",
    "        # second round\n",
    "        x_src = x[src]\n",
    "        m = torch.cat([x_src, edge_attr], dim=-1)\n",
    "        m = F.relu(self.msg2(m))\n",
    "        agg = torch.zeros_like(x)\n",
    "        agg.index_add_(0, dst, m)\n",
    "        x = F.relu(self.node_up2(x + agg))\n",
    "        return x\n",
    "\n",
    "    def forward(self, graph):\n",
    "        # graph: dict with keys x (indices), edge_index, edge_attr\n",
    "        x = self.atom_emb(graph[\"x\"])                 # [N, D]\n",
    "        x = self.message_pass(x, graph[\"edge_index\"], graph[\"edge_attr\"])\n",
    "        # global pooling (mean)\n",
    "        if x.shape[0] == 0:\n",
    "            return torch.zeros(self.atom_emb.embedding_dim, device=x.device)\n",
    "        return x.mean(dim=0)                          # [D]\n",
    "\n",
    "\n",
    "class CategoricalEmbeddings(nn.Module):\n",
    "    def __init__(self, cat_cfg):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleDict({\n",
    "            name: nn.Embedding(params[\"vocab_size\"], params[\"embed_dim\"])\n",
    "            for name, params in cat_cfg.items()\n",
    "        })\n",
    "\n",
    "    def forward(self, X_cat_dict):\n",
    "        # X_cat_dict: dict of tensors (Long) per categorical feature\n",
    "        embs = [self.embeddings[name](X_cat_dict[name]) for name in self.embeddings]\n",
    "        # If inputs are batch-sized, concatenate along last dim\n",
    "        return torch.cat([e if e.dim() > 1 else e.unsqueeze(0) for e in embs], dim=-1)\n",
    "\n",
    "\n",
    "class CandidateNetMultimodal(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.cfg = config\n",
    "\n",
    "        # Numeric projection (optional normalization outside)\n",
    "        self.num_proj = nn.Identity()\n",
    "\n",
    "        # Categorical embeddings\n",
    "        self.cat_emb = CategoricalEmbeddings(config[\"categorical\"])\n",
    "        cat_total_dim = sum([p[\"embed_dim\"] for p in config[\"categorical\"].values()])\n",
    "\n",
    "        # Formula embedding\n",
    "        self.formula_emb = FormulaEmbedder(\n",
    "            num_elements=config[\"formula\"][\"num_elements\"],\n",
    "            embed_dim=config[\"formula\"][\"embed_dim\"]\n",
    "        )\n",
    "\n",
    "        # Graph encoder\n",
    "        self.graph_enc = SimpleGraphEncoder(\n",
    "            num_elements=config[\"formula\"][\"num_elements\"],\n",
    "            atom_embed_dim=config[\"graph\"][\"atom_embed_dim\"],\n",
    "            edge_feat_dim=config[\"graph\"][\"edge_feat_dim\"]\n",
    "        )\n",
    "\n",
    "        # Fusion MLP\n",
    "        in_dim = len(config[\"numeric_cols\"]) + cat_total_dim + config[\"formula\"][\"embed_dim\"] + config[\"graph\"][\"atom_embed_dim\"]\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_dim, config[\"mlp\"][\"hidden1\"]),\n",
    "            nn.BatchNorm1d(config[\"mlp\"][\"hidden1\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=config[\"mlp\"][\"dropout\"]),\n",
    "            nn.Linear(config[\"mlp\"][\"hidden1\"], config[\"mlp\"][\"hidden2\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config[\"mlp\"][\"hidden2\"], 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, X_num, X_cat_dict, formula_counts, graph):\n",
    "        # Numeric\n",
    "        x_num = self.num_proj(X_num.float())  # [B, num_numeric] or [num_numeric]\n",
    "\n",
    "        # Categorical\n",
    "        x_cat = self.cat_emb(X_cat_dict)      # [B, cat_dim] or [cat_dim]\n",
    "\n",
    "        # Formula (composition-level; if batch, apply per-sample)\n",
    "        if formula_counts.dim() == 2:  # [B, E]\n",
    "            x_formula = torch.stack([self.formula_emb(fc) for fc in formula_counts])\n",
    "        else:  # [E]\n",
    "            x_formula = self.formula_emb(formula_counts).unsqueeze(0)\n",
    "\n",
    "        # Graph (per-sample; if batching graphs, you’d pool per sample)\n",
    "        x_graph = self.graph_enc(graph)        # [D_graph]\n",
    "        if x_graph.dim() == 1:\n",
    "            x_graph = x_graph.unsqueeze(0)\n",
    "\n",
    "        # Concatenate\n",
    "        x = torch.cat([x_num, x_cat, x_formula, x_graph], dim=-1)\n",
    "\n",
    "        # Logits\n",
    "        return self.fc(x).squeeze(-1)          # [B]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef59362",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Example training step (single sample for clarity)\u001b[39;00m\n\u001b[32m     49\u001b[39m model.train()\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m X_num, X_cat_dict, formula_counts, graph, y = \u001b[43mprepare_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m logits = model(X_num, X_cat_dict, formula_counts, graph)\n\u001b[32m     52\u001b[39m loss = criterion(logits, y)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mprepare_sample\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprepare_sample\u001b[39m(row):\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Numeric\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     X_num = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnumeric_cols\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m.unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Categorical (Long tensors)\u001b[39;00m\n\u001b[32m     12\u001b[39m     X_cat_dict = {\n\u001b[32m     13\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mspg_number\u001b[39m\u001b[33m\"\u001b[39m: torch.tensor([row[\u001b[33m\"\u001b[39m\u001b[33mspg_number_idx\u001b[39m\u001b[33m\"\u001b[39m]], dtype=torch.long, device=device),\n\u001b[32m     14\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m\"\u001b[39m: torch.tensor([row[\u001b[33m\"\u001b[39m\u001b[33mfunc_idx\u001b[39m\u001b[33m\"\u001b[39m]], dtype=torch.long, device=device),\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcrys\u001b[39m\u001b[33m\"\u001b[39m: torch.tensor([row[\u001b[33m\"\u001b[39m\u001b[33mcrys_idx\u001b[39m\u001b[33m\"\u001b[39m]], dtype=torch.long, device=device),\n\u001b[32m     18\u001b[39m     }\n",
      "\u001b[31mTypeError\u001b[39m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
     ]
    }
   ],
   "source": [
    "# Example single-sample preparation (extend to batches)\n",
    "device = torch.device(config[\"devices\"][\"model\"])\n",
    "radius = config[\"graph\"][\"radius\"]\n",
    "max_neighbors = config[\"graph\"][\"max_neighbors\"]\n",
    "\n",
    "# Prepare one sample (pseudo-code; wire this into your Dataset __getitem__)\n",
    "def prepare_sample(row):\n",
    "    # Numeric\n",
    "    X_num = torch.tensor(row[config[\"numeric_cols\"]].values, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    # Categorical (Long tensors)\n",
    "    X_cat_dict = {\n",
    "        \"spg_number\": torch.tensor([row[\"spg_number_idx\"]], dtype=torch.long, device=device),\n",
    "        \"func\": torch.tensor([row[\"func_idx\"]], dtype=torch.long, device=device),\n",
    "        \"dimensionality\": torch.tensor([row[\"dimensionality_idx\"]], dtype=torch.long, device=device),\n",
    "        \"typ\": torch.tensor([row[\"typ_idx\"]], dtype=torch.long, device=device),\n",
    "        \"crys\": torch.tensor([row[\"crys_idx\"]], dtype=torch.long, device=device),\n",
    "    }\n",
    "\n",
    "    # Formula counts vector [E]\n",
    "    # Build from parsed composition or your existing formula pipeline\n",
    "    counts = np.zeros(config[\"formula\"][\"num_elements\"], dtype=np.float32)\n",
    "    for elem, cnt in row[\"composition_counts\"].items():  # e.g., {\"Na\":1, \"I\":1}\n",
    "        idx = element_vocab[elem]\n",
    "        counts[idx] = cnt\n",
    "    formula_counts = torch.tensor(counts, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Graph from atoms_obj\n",
    "    graph = atoms_to_graph(\n",
    "        row[\"atoms_obj\"],\n",
    "        radius=radius,\n",
    "        max_neighbors=max_neighbors,\n",
    "        element_vocab=element_vocab,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Target\n",
    "    y = torch.tensor([row[\"target\"]], dtype=torch.float32, device=device)\n",
    "\n",
    "    return X_num, X_cat_dict, formula_counts, graph, y\n",
    "\n",
    "# Instantiate model\n",
    "model = CandidateNetMultimodal(config).to(device)\n",
    "pos_weight = torch.tensor([len(df[df.target==0]) / len(df[df.target==1])], device=device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Example training step (single sample for clarity)\n",
    "model.train()\n",
    "X_num, X_cat_dict, formula_counts, graph, y = prepare_sample(df_candidates.iloc[0])\n",
    "logits = model(X_num, X_cat_dict, formula_counts, graph)\n",
    "loss = criterion(logits, y)\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19f8302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
